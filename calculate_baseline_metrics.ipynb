{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c543731-5387-460b-8caa-da2e6f413078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549daa9f-e64b-436b-9e27-4d23f92054b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_data = load_dataset(\"uoft-cs/cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736d134-f847-4283-97f5-995cd53bac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10 \n",
    "\n",
    "baseline_model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "baseline_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b3487-363b-4801-a163-899402ba783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    images = [img.convert(\"RGB\") for img in examples[\"img\"]]\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "tokenized_cifar = cifar_data.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    num_proc=32 #128 cpu count\n",
    ")\n",
    "\n",
    "tokenized_cifar = tokenized_cifar.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed0271-0049-426b-b509-5c6fc7c26f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Calculating Baseline Metrics\"):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    \n",
    "    labels = batch['labels'] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    all_preds.extend(predictions)\n",
    "    all_labels.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(all_labels)\n",
    "y_pred = np.array(all_preds)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "baseline_f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Baseline F1-Score (Weighted): {baseline_f1_weighted:.4f}\")\n",
    "\n",
    "target_names = [f'Class {i}' for i in range(10)]\n",
    "\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=target_names,\n",
    "    zero_division=0 \n",
    ")\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9e50f-e6cd-419f-9d11-a1447f68a495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
