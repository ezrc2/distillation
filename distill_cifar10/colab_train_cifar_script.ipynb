{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NnsZqMhWx6d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251e934b-fc3e-479a-9be4-7e90d1c58f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/content/drive/MyDrive/project'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "DYfQ2c9tzFfT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjc9XzLKzxxN",
        "outputId": "edf3a62e-bb19-445d-d42e-4b75262dacd7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_10\t\t cifar10_distill_script.py\n",
            "cifar10_baselines.ipynb  Untitled0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 cifar10_distill_script.py --epochs 10 --lr 0.001 --resume_train ./checkpoint_10 --resume_epochs_from 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G9iMTgqz92O",
        "outputId": "0a11f80a-b1b4-453e-c817-a317b418192d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-17 19:36:38.929704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766000198.961325    4072 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766000198.970716    4072 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766000198.993625    4072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766000198.993665    4072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766000198.993671    4072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766000198.993679    4072 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 19:36:39.000076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 918/918 [00:00<00:00, 489kB/s]\n",
            "pytorch_model.bin: 100% 343M/343M [00:04<00:00, 78.2MB/s]\n",
            "preprocessor_config.json: 100% 160/160 [00:00<00:00, 1.37MB/s]\n",
            "model.safetensors:  61% 209M/343M [00:02<00:01, 118MB/s] Done loading models\n",
            "model.safetensors:  80% 276M/343M [00:03<00:00, 170MB/s]\n",
            "README.md: 5.16kB [00:00, 13.2MB/s]\n",
            "model.safetensors: 100% 343M/343M [00:03<00:00, 109MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 120M/120M [00:02<00:00, 52.7MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 23.9M/23.9M [00:01<00:00, 17.8MB/s]\n",
            "Generating train split: 100% 50000/50000 [00:00<00:00, 114020.89 examples/s]\n",
            "Generating test split: 100% 10000/10000 [00:00<00:00, 139250.35 examples/s]\n",
            "Done loading data\n",
            "0it [00:00, ?it/s]Epoch 11 [1/1563], Loss: 0.2287\n",
            "100it [00:43,  2.39it/s]Epoch 11 [101/1563], Loss: 0.2312\n",
            "200it [01:25,  2.34it/s]Epoch 11 [201/1563], Loss: 0.1302\n",
            "300it [02:08,  2.30it/s]Epoch 11 [301/1563], Loss: 0.1836\n",
            "400it [02:52,  2.25it/s]Epoch 11 [401/1563], Loss: 0.3769\n",
            "500it [03:37,  2.22it/s]Epoch 11 [501/1563], Loss: 0.0817\n",
            "600it [04:22,  2.21it/s]Epoch 11 [601/1563], Loss: 0.2325\n",
            "700it [05:08,  2.15it/s]Epoch 11 [701/1563], Loss: 0.0651\n",
            "800it [05:54,  2.18it/s]Epoch 11 [801/1563], Loss: 0.9271\n",
            "900it [06:40,  2.17it/s]Epoch 11 [901/1563], Loss: 0.2507\n",
            "1000it [07:26,  2.17it/s]Epoch 11 [1001/1563], Loss: 0.3783\n",
            "1100it [08:12,  2.18it/s]Epoch 11 [1101/1563], Loss: 0.2611\n",
            "1200it [08:59,  2.18it/s]Epoch 11 [1201/1563], Loss: 0.1949\n",
            "1300it [09:45,  2.19it/s]Epoch 11 [1301/1563], Loss: 0.3271\n",
            "1400it [10:31,  2.18it/s]Epoch 11 [1401/1563], Loss: 0.5912\n",
            "1500it [11:17,  2.17it/s]Epoch 11 [1501/1563], Loss: 0.0799\n",
            "1563it [11:46,  2.21it/s]\n",
            "---------------------------------------\n",
            "epoch 11 done:\n",
            "avg train loss: 0.2258\n",
            "val acc: 0.9223\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 12 [1/1563], Loss: 0.1153\n",
            "100it [00:46,  2.19it/s]Epoch 12 [101/1563], Loss: 0.1756\n",
            "200it [01:32,  2.18it/s]Epoch 12 [201/1563], Loss: 0.2377\n",
            "300it [02:18,  2.18it/s]Epoch 12 [301/1563], Loss: 0.2325\n",
            "400it [03:04,  2.16it/s]Epoch 12 [401/1563], Loss: 0.0622\n",
            "500it [03:50,  2.18it/s]Epoch 12 [501/1563], Loss: 0.0740\n",
            "600it [04:36,  2.16it/s]Epoch 12 [601/1563], Loss: 0.1703\n",
            "700it [05:22,  2.18it/s]Epoch 12 [701/1563], Loss: 0.1326\n",
            "800it [06:08,  2.17it/s]Epoch 12 [801/1563], Loss: 0.0872\n",
            "900it [06:54,  2.18it/s]Epoch 12 [901/1563], Loss: 0.4166\n",
            "1000it [07:41,  2.17it/s]Epoch 12 [1001/1563], Loss: 0.2978\n",
            "1100it [08:27,  2.18it/s]Epoch 12 [1101/1563], Loss: 0.1135\n",
            "1200it [09:13,  2.17it/s]Epoch 12 [1201/1563], Loss: 0.3199\n",
            "1300it [09:59,  2.17it/s]Epoch 12 [1301/1563], Loss: 0.3601\n",
            "1400it [10:45,  2.18it/s]Epoch 12 [1401/1563], Loss: 0.0709\n",
            "1500it [11:31,  2.17it/s]Epoch 12 [1501/1563], Loss: 0.2178\n",
            "1563it [12:00,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 12 done:\n",
            "avg train loss: 0.1757\n",
            "val acc: 0.9307\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 13 [1/1563], Loss: 0.0810\n",
            "100it [00:46,  2.18it/s]Epoch 13 [101/1563], Loss: 0.2607\n",
            "200it [01:32,  2.19it/s]Epoch 13 [201/1563], Loss: 0.1081\n",
            "300it [02:18,  2.17it/s]Epoch 13 [301/1563], Loss: 0.2901\n",
            "400it [03:04,  2.18it/s]Epoch 13 [401/1563], Loss: 0.2010\n",
            "500it [03:50,  2.15it/s]Epoch 13 [501/1563], Loss: 0.1762\n",
            "600it [04:36,  2.18it/s]Epoch 13 [601/1563], Loss: 0.1773\n",
            "700it [05:23,  2.18it/s]Epoch 13 [701/1563], Loss: 0.1461\n",
            "800it [06:09,  2.17it/s]Epoch 13 [801/1563], Loss: 0.0868\n",
            "900it [06:55,  2.15it/s]Epoch 13 [901/1563], Loss: 0.0974\n",
            "1000it [07:41,  2.18it/s]Epoch 13 [1001/1563], Loss: 0.1699\n",
            "1100it [08:27,  2.19it/s]Epoch 13 [1101/1563], Loss: 0.1420\n",
            "1200it [09:13,  2.17it/s]Epoch 13 [1201/1563], Loss: 0.0662\n",
            "1300it [09:59,  2.17it/s]Epoch 13 [1301/1563], Loss: 0.0708\n",
            "1400it [10:45,  2.17it/s]Epoch 13 [1401/1563], Loss: 0.1072\n",
            "1500it [11:31,  2.18it/s]Epoch 13 [1501/1563], Loss: 0.0635\n",
            "1563it [12:00,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 13 done:\n",
            "avg train loss: 0.1555\n",
            "val acc: 0.9213\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 14 [1/1563], Loss: 0.1527\n",
            "100it [00:46,  2.18it/s]Epoch 14 [101/1563], Loss: 0.1200\n",
            "200it [01:32,  2.18it/s]Epoch 14 [201/1563], Loss: 0.2180\n",
            "300it [02:18,  2.18it/s]Epoch 14 [301/1563], Loss: 0.1675\n",
            "400it [03:04,  2.17it/s]Epoch 14 [401/1563], Loss: 0.0696\n",
            "500it [03:50,  2.17it/s]Epoch 14 [501/1563], Loss: 0.1545\n",
            "600it [04:36,  2.17it/s]Epoch 14 [601/1563], Loss: 0.1422\n",
            "700it [05:22,  2.18it/s]Epoch 14 [701/1563], Loss: 0.1171\n",
            "800it [06:09,  2.17it/s]Epoch 14 [801/1563], Loss: 0.0430\n",
            "900it [06:55,  2.19it/s]Epoch 14 [901/1563], Loss: 0.0645\n",
            "1000it [07:41,  2.15it/s]Epoch 14 [1001/1563], Loss: 0.0706\n",
            "1100it [08:27,  2.18it/s]Epoch 14 [1101/1563], Loss: 0.0381\n",
            "1200it [09:13,  2.18it/s]Epoch 14 [1201/1563], Loss: 0.0953\n",
            "1300it [09:59,  2.17it/s]Epoch 14 [1301/1563], Loss: 0.0765\n",
            "1400it [10:45,  2.19it/s]Epoch 14 [1401/1563], Loss: 0.0450\n",
            "1500it [11:31,  2.18it/s]Epoch 14 [1501/1563], Loss: 0.2927\n",
            "1563it [12:00,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 14 done:\n",
            "avg train loss: 0.1456\n",
            "val acc: 0.9275\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 15 [1/1563], Loss: 0.0764\n",
            "100it [00:46,  2.18it/s]Epoch 15 [101/1563], Loss: 0.0897\n",
            "200it [01:32,  2.18it/s]Epoch 15 [201/1563], Loss: 0.0537\n",
            "300it [02:18,  2.18it/s]Epoch 15 [301/1563], Loss: 0.0665\n",
            "400it [03:04,  2.18it/s]Epoch 15 [401/1563], Loss: 0.1346\n",
            "500it [03:50,  2.17it/s]Epoch 15 [501/1563], Loss: 0.0529\n",
            "600it [04:36,  2.18it/s]Epoch 15 [601/1563], Loss: 0.0578\n",
            "700it [05:22,  2.17it/s]Epoch 15 [701/1563], Loss: 0.0606\n",
            "800it [06:08,  2.18it/s]Epoch 15 [801/1563], Loss: 0.2421\n",
            "900it [06:54,  2.16it/s]Epoch 15 [901/1563], Loss: 0.2237\n",
            "1000it [07:40,  2.17it/s]Epoch 15 [1001/1563], Loss: 0.0736\n",
            "1100it [08:27,  2.16it/s]Epoch 15 [1101/1563], Loss: 0.0725\n",
            "1200it [09:13,  2.18it/s]Epoch 15 [1201/1563], Loss: 0.2070\n",
            "1300it [09:59,  2.18it/s]Epoch 15 [1301/1563], Loss: 0.1214\n",
            "1400it [10:45,  2.18it/s]Epoch 15 [1401/1563], Loss: 0.1760\n",
            "1500it [11:31,  2.18it/s]Epoch 15 [1501/1563], Loss: 0.1707\n",
            "1563it [12:00,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 15 done:\n",
            "avg train loss: 0.1295\n",
            "val acc: 0.9277\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 16 [1/1563], Loss: 0.0432\n",
            "100it [00:46,  2.17it/s]Epoch 16 [101/1563], Loss: 0.0602\n",
            "200it [01:32,  2.18it/s]Epoch 16 [201/1563], Loss: 0.0957\n",
            "300it [02:18,  2.18it/s]Epoch 16 [301/1563], Loss: 0.1316\n",
            "400it [03:04,  2.18it/s]Epoch 16 [401/1563], Loss: 0.0728\n",
            "500it [03:50,  2.19it/s]Epoch 16 [501/1563], Loss: 0.3599\n",
            "600it [04:36,  2.15it/s]Epoch 16 [601/1563], Loss: 0.1749\n",
            "700it [05:22,  2.17it/s]Epoch 16 [701/1563], Loss: 0.2042\n",
            "800it [06:08,  2.15it/s]Epoch 16 [801/1563], Loss: 0.0644\n",
            "900it [06:54,  2.19it/s]Epoch 16 [901/1563], Loss: 0.0474\n",
            "1000it [07:40,  2.18it/s]Epoch 16 [1001/1563], Loss: 0.0483\n",
            "1100it [08:26,  2.17it/s]Epoch 16 [1101/1563], Loss: 0.2567\n",
            "1200it [09:12,  2.17it/s]Epoch 16 [1201/1563], Loss: 0.2424\n",
            "1300it [09:58,  2.18it/s]Epoch 16 [1301/1563], Loss: 0.1467\n",
            "1400it [10:44,  2.17it/s]Epoch 16 [1401/1563], Loss: 0.1460\n",
            "1500it [11:31,  2.19it/s]Epoch 16 [1501/1563], Loss: 0.2304\n",
            "1563it [11:59,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 16 done:\n",
            "avg train loss: 0.1243\n",
            "val acc: 0.9256\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 17 [1/1563], Loss: 0.2002\n",
            "100it [00:46,  2.18it/s]Epoch 17 [101/1563], Loss: 0.0649\n",
            "200it [01:32,  2.17it/s]Epoch 17 [201/1563], Loss: 0.0570\n",
            "300it [02:18,  2.18it/s]Epoch 17 [301/1563], Loss: 0.1017\n",
            "400it [03:04,  2.15it/s]Epoch 17 [401/1563], Loss: 0.0565\n",
            "500it [03:50,  2.17it/s]Epoch 17 [501/1563], Loss: 0.0950\n",
            "600it [04:36,  2.17it/s]Epoch 17 [601/1563], Loss: 0.1027\n",
            "700it [05:22,  2.18it/s]Epoch 17 [701/1563], Loss: 0.0443\n",
            "800it [06:08,  2.18it/s]Epoch 17 [801/1563], Loss: 0.0479\n",
            "900it [06:54,  2.18it/s]Epoch 17 [901/1563], Loss: 0.1203\n",
            "1000it [07:40,  2.18it/s]Epoch 17 [1001/1563], Loss: 0.1169\n",
            "1100it [08:26,  2.18it/s]Epoch 17 [1101/1563], Loss: 0.0602\n",
            "1200it [09:12,  2.17it/s]Epoch 17 [1201/1563], Loss: 0.1647\n",
            "1300it [09:59,  2.18it/s]Epoch 17 [1301/1563], Loss: 0.0563\n",
            "1400it [10:45,  2.18it/s]Epoch 17 [1401/1563], Loss: 0.1510\n",
            "1500it [11:31,  2.17it/s]Epoch 17 [1501/1563], Loss: 0.2599\n",
            "1563it [12:00,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 17 done:\n",
            "avg train loss: 0.1164\n",
            "val acc: 0.9296\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 18 [1/1563], Loss: 0.1085\n",
            "100it [00:46,  2.17it/s]Epoch 18 [101/1563], Loss: 0.0686\n",
            "200it [01:32,  2.18it/s]Epoch 18 [201/1563], Loss: 0.0417\n",
            "300it [02:18,  2.17it/s]Epoch 18 [301/1563], Loss: 0.0668\n",
            "400it [03:04,  2.17it/s]Epoch 18 [401/1563], Loss: 0.0966\n",
            "500it [03:50,  2.17it/s]Epoch 18 [501/1563], Loss: 0.0849\n",
            "600it [04:36,  2.17it/s]Epoch 18 [601/1563], Loss: 0.0372\n",
            "700it [05:22,  2.18it/s]Epoch 18 [701/1563], Loss: 0.0480\n",
            "800it [06:08,  2.18it/s]Epoch 18 [801/1563], Loss: 0.2430\n",
            "900it [06:55,  2.18it/s]Epoch 18 [901/1563], Loss: 0.1310\n",
            "1000it [07:41,  2.17it/s]Epoch 18 [1001/1563], Loss: 0.1468\n",
            "1100it [08:27,  2.18it/s]Epoch 18 [1101/1563], Loss: 0.2318\n",
            "1200it [09:13,  2.18it/s]Epoch 18 [1201/1563], Loss: 0.2847\n",
            "1300it [09:59,  2.17it/s]Epoch 18 [1301/1563], Loss: 0.0632\n",
            "1400it [10:45,  2.18it/s]Epoch 18 [1401/1563], Loss: 0.0588\n",
            "1500it [11:31,  2.15it/s]Epoch 18 [1501/1563], Loss: 0.1752\n",
            "1563it [12:00,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 18 done:\n",
            "avg train loss: 0.1084\n",
            "val acc: 0.9200\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 19 [1/1563], Loss: 0.0474\n",
            "100it [00:46,  2.17it/s]Epoch 19 [101/1563], Loss: 0.0608\n",
            "200it [01:32,  2.19it/s]Epoch 19 [201/1563], Loss: 0.0313\n",
            "300it [02:18,  2.18it/s]Epoch 19 [301/1563], Loss: 0.0643\n",
            "400it [03:04,  2.18it/s]Epoch 19 [401/1563], Loss: 0.0543\n",
            "500it [03:50,  2.18it/s]Epoch 19 [501/1563], Loss: 0.0783\n",
            "600it [04:36,  2.18it/s]Epoch 19 [601/1563], Loss: 0.1267\n",
            "700it [05:22,  2.16it/s]Epoch 19 [701/1563], Loss: 0.0490\n",
            "800it [06:08,  2.18it/s]Epoch 19 [801/1563], Loss: 0.1688\n",
            "900it [06:54,  2.16it/s]Epoch 19 [901/1563], Loss: 0.0712\n",
            "1000it [07:40,  2.19it/s]Epoch 19 [1001/1563], Loss: 0.1382\n",
            "1100it [08:26,  2.18it/s]Epoch 19 [1101/1563], Loss: 0.1361\n",
            "1200it [09:12,  2.17it/s]Epoch 19 [1201/1563], Loss: 0.1246\n",
            "1300it [09:58,  2.18it/s]Epoch 19 [1301/1563], Loss: 0.0967\n",
            "1400it [10:44,  2.18it/s]Epoch 19 [1401/1563], Loss: 0.0483\n",
            "1500it [11:30,  2.18it/s]Epoch 19 [1501/1563], Loss: 0.1044\n",
            "1563it [11:59,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 19 done:\n",
            "avg train loss: 0.1075\n",
            "val acc: 0.9265\n",
            "---------------------------------------\n",
            "0it [00:00, ?it/s]Epoch 20 [1/1563], Loss: 0.2857\n",
            "100it [00:46,  2.18it/s]Epoch 20 [101/1563], Loss: 0.1670\n",
            "200it [01:32,  2.19it/s]Epoch 20 [201/1563], Loss: 0.0725\n",
            "300it [02:18,  2.19it/s]Epoch 20 [301/1563], Loss: 0.0769\n",
            "400it [03:04,  2.18it/s]Epoch 20 [401/1563], Loss: 0.1519\n",
            "500it [03:50,  2.18it/s]Epoch 20 [501/1563], Loss: 0.1342\n",
            "600it [04:36,  2.18it/s]Epoch 20 [601/1563], Loss: 0.5814\n",
            "700it [05:22,  2.18it/s]Epoch 20 [701/1563], Loss: 0.0410\n",
            "800it [06:08,  2.17it/s]Epoch 20 [801/1563], Loss: 0.1247\n",
            "900it [06:54,  2.18it/s]Epoch 20 [901/1563], Loss: 0.0921\n",
            "1000it [07:40,  2.17it/s]Epoch 20 [1001/1563], Loss: 0.0237\n",
            "1100it [08:26,  2.18it/s]Epoch 20 [1101/1563], Loss: 0.0879\n",
            "1200it [09:12,  2.15it/s]Epoch 20 [1201/1563], Loss: 0.0736\n",
            "1300it [09:58,  2.18it/s]Epoch 20 [1301/1563], Loss: 0.1331\n",
            "1400it [10:44,  2.17it/s]Epoch 20 [1401/1563], Loss: 0.1029\n",
            "1500it [11:30,  2.18it/s]Epoch 20 [1501/1563], Loss: 0.0392\n",
            "1563it [11:59,  2.17it/s]\n",
            "---------------------------------------\n",
            "epoch 20 done:\n",
            "avg train loss: 0.1030\n",
            "val acc: 0.9297\n",
            "---------------------------------------\n",
            "modelsaved to checkpoint_10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kh-0pF5Z0Ea-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}