{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7476827-7854-4590-b8b8-668805148eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b74b0cc-a4c4-4a42-b87d-03d89a101592",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e481977-5b70-468b-9c5a-dc9e0580216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3455f64-ae7f-4f4a-a7c2-c7a506143f11",
   "metadata": {},
   "source": [
    "## ViT classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175cacad-a740-42a7-a062-b790b2b4086f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224\"\n",
    "model = ViTForImageClassification.from_pretrained(model_id)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd45b51-76c6-49d3-a882-1e0fc8aa9709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e752a5d5e04c4991a6574c0271ce00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    images = [img.convert(\"RGB\") for img in examples[\"image\"]]\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690b5a31-9b4c-418a-9562-fcb3848a3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(tokenized_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "815f94a7-b4b7-43f8-b3f8-f1f7a6b48e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8f1408b69a462fa144bf2946285632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    all_preds.extend(predictions)\n",
    "    all_labels.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd223d5a-cbb0-4d0f-b1c6-721ab882c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02666347 not in map\n",
      "n03373237 not in map\n",
      "n04465666 not in map\n",
      "n04598010 not in map\n",
      "n07056680 not in map\n",
      "n07646821 not in map\n",
      "n07647870 not in map\n",
      "n07657664 not in map\n",
      "n07975909 not in map\n",
      "n08496334 not in map\n",
      "n08620881 not in map\n",
      "n08742578 not in map\n",
      "n12520864 not in map\n",
      "n13001041 not in map\n",
      "n13652335 not in map\n",
      "n13652994 not in map\n",
      "n13719102 not in map\n",
      "n14991210 not in map\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  1,  25,  30,  32,  50,  61,  69,  71,  75,  76,  79, 105, 107, 109,\n",
       "        113, 115, 122, 123, 128, 145, 146, 149, 187, 207, 208, 235, 267, 281,\n",
       "        283, 285, 286, 291, 294, 301, 311, 313, 314, 315, 319, 323, 325, 329,\n",
       "        338, 341, 345, 347, 349, 353, 354, 365, 367, 372, 386, 387,  -1, 400,\n",
       "        406, 414, 421, 424, 425, 427, 430, 435, 436, 437, 438, 440, 445, 447,\n",
       "        448, 457, 458, 463, 466, 467, 470, 471, 474, 480, 485, 492, 496, 500,\n",
       "        508, 509, 511, 517, 525, 526, 532, 543, 557,  -1, 562, 565, 567, 568,\n",
       "        570, 573, 576, 604, 605, 612, 614, 619, 621, 625, 627, 635, 645, 652,\n",
       "        655, 675, 678, 682, 683, 687, 704, 707, 716, 720, 731, 734, 735, 737,\n",
       "        739, 744, 747, 760, 761, 765, 768, 774, 779, 781, 786, 801, 806, 808,\n",
       "        811, 815, 817, 821, 826, 837, 839, 842, 845, 849, 850, 853, 862,  -1,\n",
       "        873, 874, 877, 879, 887, 888, 890, 899, 900, 909,  -1, 917,  -1, 924,\n",
       "        928, 929,  -1,  -1,  -1, 932, 935, 938, 945, 951, 954, 957, 962, 963,\n",
       "        964, 967,  -1,  -1,  -1,  -1, 970, 972, 973, 975, 978, 988,  -1,  -1,\n",
       "         -1,  -1,  -1,  -1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to map Tiny Imagenet to Imagenet labels since there is 200 vs 1000 classes\n",
    "# https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('imagenet_class_index.json', 'r') as f:\n",
    "    imagenet_index_data = json.load(f)\n",
    "\n",
    "wnid_to_idx = {v[0]: int(k) for k, v in imagenet_index_data.items()}\n",
    "\n",
    "tiny_imagenet_wnids = dataset.features['label'].names\n",
    "\n",
    "tiny_to_vit_idx = []\n",
    "for i, wnid in enumerate(tiny_imagenet_wnids):\n",
    "    if wnid in wnid_to_idx:\n",
    "        tiny_to_vit_idx.append(wnid_to_idx[wnid])\n",
    "    else:\n",
    "        print(wnid, 'not in map')\n",
    "        tiny_to_vit_idx.append(-1)\n",
    "\n",
    "mapping = torch.tensor(tiny_to_vit_idx)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1de81969-d82e-4b28-b665-4bde93d8f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6291208791208791\n",
      "weighted f1: 0.7241718171110046\n"
     ]
    }
   ],
   "source": [
    "y_true_tiny = torch.tensor(all_labels)\n",
    "y_true_full = mapping[y_true_tiny].numpy()\n",
    "\n",
    "y_true_valid = y_true_full[y_true_full != -1]\n",
    "y_pred_valid = np.array(all_preds)[y_true_full != -1]\n",
    "\n",
    "accuracy = accuracy_score(y_true_valid, y_pred_valid)\n",
    "print(f'accuracy: {accuracy}')\n",
    "\n",
    "f1 = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "print(f\"weighted f1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c7b1f-2543-40c3-8dfb-4515172624af",
   "metadata": {},
   "source": [
    "These metrics seem low but it's because we are testing the 1000 set model against a 200 class test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcea404-8a43-4079-98d0-9723ce9ba550",
   "metadata": {},
   "source": [
    "## MobileNetv2 classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "696e3444-fed2-41a6-8913-872b5bb20a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "795d814f-9d4f-428c-855d-5b393158853f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec22a0bed8314b37be414fdb8c40ef0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/406 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/mobilenet_v2_1.0_224\"\n",
    "model = AutoModelForImageClassification.from_pretrained(model_id)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1daa298e-48df-4736-9101-59b7ffb30d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85113d1b196a4e56b593e28ac084a82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    images = [img.convert(\"RGB\") for img in examples[\"image\"]]\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae255a1d-0683-45df-8f2e-a04b7fdc60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(tokenized_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20db6cfa-58a1-4442-bf42-35096eec0350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55bf823fe7c4a6397a030092f574677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds2 = []\n",
    "all_labels2 = []\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    all_preds2.extend(predictions)\n",
    "    all_labels2.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02a6f8f9-660c-4e9e-b2db-4a1b30a6b99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02666347 not in map\n",
      "n03373237 not in map\n",
      "n04465666 not in map\n",
      "n04598010 not in map\n",
      "n07056680 not in map\n",
      "n07646821 not in map\n",
      "n07647870 not in map\n",
      "n07657664 not in map\n",
      "n07975909 not in map\n",
      "n08496334 not in map\n",
      "n08620881 not in map\n",
      "n08742578 not in map\n",
      "n12520864 not in map\n",
      "n13001041 not in map\n",
      "n13652335 not in map\n",
      "n13652994 not in map\n",
      "n13719102 not in map\n",
      "n14991210 not in map\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  2,  26,  31,  33,  51,  62,  70,  72,  76,  77,  80, 106, 108, 110,\n",
       "        114, 116, 123, 124, 129, 146, 147, 150, 188, 208, 209, 236, 268, 282,\n",
       "        284, 286, 287, 292, 295, 302, 312, 314, 315, 316, 320, 324, 326, 330,\n",
       "        339, 342, 346, 348, 350, 354, 355, 366, 368, 373, 387, 388,  -1, 401,\n",
       "        407, 415, 422, 425, 426, 428, 431, 436, 437, 438, 439, 441, 446, 448,\n",
       "        449, 458, 459, 464, 467, 468, 471, 472, 475, 481, 486, 493, 497, 501,\n",
       "        509, 510, 512, 518, 526, 527, 533, 544, 558,  -1, 563, 566, 568, 569,\n",
       "        571, 574, 577, 605, 606, 613, 615, 620, 622, 626, 628, 636, 646, 653,\n",
       "        656, 676, 679, 683, 684, 688, 705, 708, 717, 721, 732, 735, 736, 738,\n",
       "        740, 745, 748, 761, 762, 766, 769, 775, 780, 782, 787, 802, 807, 809,\n",
       "        812, 816, 818, 822, 827, 838, 840, 843, 846, 850, 851, 854, 863,  -1,\n",
       "        874, 875, 878, 880, 888, 889, 891, 900, 901, 910,  -1, 918,  -1, 925,\n",
       "        929, 930,  -1,  -1,  -1, 933, 936, 939, 946, 952, 955, 958, 963, 964,\n",
       "        965, 968,  -1,  -1,  -1,  -1, 971, 973, 974, 976, 979, 989,  -1,  -1,\n",
       "         -1,  -1,  -1,  -1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.config.id2label has every class shifted up by 1\n",
    "tiny_to_vit_idx = []\n",
    "for i, wnid in enumerate(tiny_imagenet_wnids):\n",
    "    if wnid in wnid_to_idx:\n",
    "        tiny_to_vit_idx.append(wnid_to_idx[wnid] + 1)\n",
    "    else:\n",
    "        print(wnid, 'not in map')\n",
    "        tiny_to_vit_idx.append(-1)\n",
    "\n",
    "mapping = torch.tensor(tiny_to_vit_idx)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7cf9c833-4c7b-4ec1-ab57-9e41b323d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.19648351648351647\n",
      "weighted f1: 0.2754208517960667\n"
     ]
    }
   ],
   "source": [
    "y_true_tiny = torch.tensor(all_labels2)\n",
    "y_true_full = mapping[y_true_tiny].numpy()\n",
    "\n",
    "y_true_valid = y_true_full[y_true_full != -1]\n",
    "y_pred_valid = np.array(all_preds2)[y_true_full != -1]\n",
    "\n",
    "accuracy = accuracy_score(y_true_valid, y_pred_valid)\n",
    "print(f'accuracy: {accuracy}')\n",
    "\n",
    "f1 = f1_score(y_true_valid, y_pred_valid, average='weighted')\n",
    "print(f\"weighted f1: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
